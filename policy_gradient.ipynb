{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv\n",
    "from utils import select_action, get_state_tensor\n",
    "from TaxiMemory import TaxiMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_observations, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.layers(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent():\n",
    "    def __init__(self, n_observations, n_actions, device) -> None:\n",
    "        self.policy_net = PolicyNet(n_observations, n_actions).to(device)\n",
    "    \n",
    "    def get_action(self, state: torch.tensor) -> tuple[int, float]:\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 300\n",
    "MAX_FUEL = 50\n",
    "LR = 0.0001\n",
    "gamma = 0.99\n",
    "EPS = np.finfo(np.float32).eps.item()\n",
    "\n",
    "log_freq = 20\n",
    "save_freq = 250\n",
    "partial_prob = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "n_observations = 23\n",
    "n_actions = 6\n",
    "# NN\n",
    "agent = PGAgent(n_observations, n_actions, device=device)\n",
    "agent.policy_net.load_state_dict(torch.load(\"pg1000.pth\"))\n",
    "optimizer = optim.AdamW(agent.policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "taxi_memory = TaxiMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_jump_before_pick(env, taxi_memory, k=1):\n",
    "    '''\n",
    "    k=0-3\n",
    "    '''\n",
    "    if k == 0:\n",
    "        return env.get_state()\n",
    "    dest_idx = env.stations.index(env.destination)\n",
    "    locations_idx = list(range(4)) # all\n",
    "    locations_idx.remove(env.stations.index(env.passenger_loc)) # remove goal\n",
    "    locations_idx = random.sample(locations_idx, k) # shrink to k\n",
    "\n",
    "    env.taxi_pos = env.stations[random.choice(locations_idx)]\n",
    "\n",
    "    for idx in locations_idx:\n",
    "        taxi_memory.visit_mask[idx] = 0\n",
    "    if dest_idx in locations_idx:\n",
    "        taxi_memory.destination_mask[dest_idx] = 1\n",
    "    return env.get_state()\n",
    "\n",
    "def env_jump_after_pick(env, taxi_memory, k=1):\n",
    "    '''\n",
    "    k=0-3\n",
    "    '''\n",
    "    if k == 0:\n",
    "        env.get_state()\n",
    "    dest_idx = env.stations.index(env.destination)\n",
    "    passenger_idx = env.stations.index(env.passenger_loc)\n",
    "    locations_idx = list(range(4)) # all\n",
    "    locations_idx.remove(passenger_idx) # remove passenger and add it back later\n",
    "    locations_idx = random.sample(locations_idx, k-1) # shrink to k-1\n",
    "    locations_idx.append(passenger_idx)\n",
    "\n",
    "    env.taxi_pos = env.passenger_loc\n",
    "    env.step(4)\n",
    "\n",
    "    taxi_memory.passenger_picked_up = True\n",
    "    for idx in locations_idx:\n",
    "        taxi_memory.visit_mask[idx] = 0\n",
    "    if dest_idx in locations_idx:\n",
    "        taxi_memory.destination_mask[dest_idx] = 1\n",
    "    return env.get_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\t, Average Score: -4.90\n",
      "Episode 20\t, Average Score: -7.04\n",
      "Episode 40\t, Average Score: -6.97\n",
      "Episode 60\t, Average Score: -6.95\n",
      "Episode 80\t, Average Score: -6.63\n",
      "Episode 100\t, Average Score: -6.65\n",
      "Episode 120\t, Average Score: -6.40\n",
      "Episode 140\t, Average Score: -6.25\n",
      "Episode 160\t, Average Score: -6.30\n",
      "Episode 180\t, Average Score: -6.40\n",
      "Episode 200\t, Average Score: -6.55\n",
      "Episode 220\t, Average Score: -6.45\n",
      "Episode 240\t, Average Score: -6.85\n",
      "Episode 260\t, Average Score: -6.75\n",
      "Episode 280\t, Average Score: -7.40\n"
     ]
    }
   ],
   "source": [
    "episodice_rewards = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    use_partial = random.random() < partial_prob\n",
    "    if use_partial:\n",
    "        env = SimpleTaxiEnv(fuel_limit=MAX_FUEL, partial=True)\n",
    "    else:\n",
    "        env = SimpleTaxiEnv(fuel_limit=MAX_FUEL, partial=False)\n",
    "    state, info = env.reset()\n",
    "    taxi_memory.reset(state)\n",
    "    if use_partial:\n",
    "        twist_level = np.random.randint(0, 4)\n",
    "        state = env_jump_before_pick(env, taxi_memory, twist_level) # spectial setting\n",
    "    else:\n",
    "        twist_level = np.random.randint(1, 4)\n",
    "        state = env_jump_after_pick(env, taxi_memory, twist_level) # spectial setting\n",
    "    state = get_state_tensor(state, taxi_memory, device)\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    terminated = False\n",
    "\n",
    "    for step in count():\n",
    "        action, log_prob = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = get_state_tensor(next_state, taxi_memory, device, action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            terminated = step < MAX_FUEL - 1\n",
    "            break\n",
    "        state = next_state\n",
    "    total_reward = sum(rewards)\n",
    "    episodice_rewards.append(total_reward)\n",
    "    scores_deque.append(total_reward)\n",
    "    \n",
    "    returns = []\n",
    "    G = 0 if terminated else 0.1 * MAX_FUEL\n",
    "    for reward in rewards[::-1]:\n",
    "        G = reward + gamma * G\n",
    "        returns.append(G)\n",
    "    returns = torch.tensor(returns[::-1])\n",
    "    returns = (returns - returns.mean()) / (returns.std() + EPS)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, disc_return in zip(log_probs, returns):\n",
    "        policy_loss.append(-log_prob * disc_return)\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i_episode % log_freq == 0:\n",
    "        print(f'Episode {i_episode}\\t, Average Score: {np.mean(scores_deque):.2f}')\n",
    "    if i_episode % save_freq == 0:\n",
    "        torch.save(agent.policy_net.state_dict(), f\"pg_ckpt-{i_episode}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net.state_dict(), f\"pg1000.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
