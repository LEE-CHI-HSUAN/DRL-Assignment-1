{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv\n",
    "from utils import select_action, get_state_tensor\n",
    "from TaxiMemory import TaxiMemory\n",
    "from dqn_net import DQN\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.2\n",
    "TAU = 0.001\n",
    "LR = 1e-4\n",
    "MAX_FUEL = 50\n",
    "num_episodes = 1000\n",
    "log_steps = 100\n",
    "partial_prob = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = SimpleTaxiEnv(fuel_limit=MAX_FUEL)\n",
    "# state, info = env.reset()\n",
    "n_observations = 23\n",
    "n_actions = 6\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "# policy_net.load_state_dict(torch.load(\"DQN.pt\"))\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "taxi_memory = TaxiMemory()\n",
    "\n",
    "\n",
    "def get_epsilon(epsilon_start, epsilon_end, total, current):\n",
    "    adjust_term = -epsilon_end * current / total\n",
    "    k = math.log(epsilon_start / epsilon_end) / total\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * (\n",
    "        math.exp(-k * current) + adjust_term\n",
    "    )\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "episodic_rewards = []\n",
    "\n",
    "\n",
    "def plot_durations_and_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    rewards_t = torch.tensor(episodic_rewards, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Training...\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    # plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy(), label=\"duration\")\n",
    "    plt.plot(rewards_t.numpy(), label=\"rewards\")\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label=\"smooth duration\")\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label=\"smooth rewards\")\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = (\n",
    "            target_net(non_final_next_states).max(1).values\n",
    "        )\n",
    "    td_target = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, td_target.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_jump_before_pick(env, taxi_memory, k=1):\n",
    "    '''\n",
    "    k=0-3\n",
    "    '''\n",
    "    if k == 0:\n",
    "        return env.get_state()\n",
    "    dest_idx = env.stations.index(env.destination)\n",
    "    locations_idx = list(range(4)) # all\n",
    "    locations_idx.remove(env.stations.index(env.passenger_loc)) # remove goal\n",
    "    locations_idx = random.sample(locations_idx, k) # shrink to k\n",
    "\n",
    "    env.taxi_pos = env.stations[random.choice(locations_idx)]\n",
    "\n",
    "    for idx in locations_idx:\n",
    "        taxi_memory.visit_mask[idx] = 0\n",
    "    if dest_idx in locations_idx:\n",
    "        taxi_memory.destination_mask[dest_idx] = 1\n",
    "    return env.get_state()\n",
    "\n",
    "def env_jump_after_pick(env, taxi_memory, k=1):\n",
    "    '''\n",
    "    k=0-3\n",
    "    '''\n",
    "    if k == 0:\n",
    "        env.get_state()\n",
    "    dest_idx = env.stations.index(env.destination)\n",
    "    passenger_idx = env.stations.index(env.passenger_loc)\n",
    "    locations_idx = list(range(4)) # all\n",
    "    locations_idx.remove(passenger_idx) # remove passenger and add it back later\n",
    "    locations_idx = random.sample(locations_idx, k-1) # shrink to k-1\n",
    "    locations_idx.append(passenger_idx)\n",
    "\n",
    "    env.taxi_pos = env.passenger_loc\n",
    "    env.step(4)\n",
    "\n",
    "    taxi_memory.passenger_picked_up = True\n",
    "    for idx in locations_idx:\n",
    "        taxi_memory.visit_mask[idx] = 0\n",
    "    if dest_idx in locations_idx:\n",
    "        taxi_memory.destination_mask[dest_idx] = 1\n",
    "    return env.get_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleTaxiEnv(fuel_limit=MAX_FUEL, partial=True)\n",
    "state, info = env.reset()\n",
    "taxi_memory.reset(state)\n",
    "twist_level = np.random.randint(0, 4)\n",
    "print(twist_level)\n",
    "state = env_jump_before_pick(env, taxi_memory, twist_level) # spectial setting\n",
    "get_state_tensor(state, taxi_memory, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleTaxiEnv(fuel_limit=MAX_FUEL, partial=False)\n",
    "state, info = env.reset()\n",
    "print(env.passenger_loc)\n",
    "taxi_memory.reset(state)\n",
    "twist_level = np.random.randint(1, 4)\n",
    "print(twist_level)\n",
    "state = env_jump_after_pick(env, taxi_memory, twist_level)\n",
    "get_state_tensor(state, taxi_memory, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    # print(\"RESET-------------\")\n",
    "    epsilon = get_epsilon(EPS_START, EPS_END, num_episodes, i_episode)\n",
    "    total_reward = 0\n",
    "    use_partial = random.random() < partial_prob\n",
    "    if use_partial:\n",
    "        env = SimpleTaxiEnv(fuel_limit=MAX_FUEL, partial=True)\n",
    "    else:\n",
    "        env = SimpleTaxiEnv(fuel_limit=MAX_FUEL, partial=False)\n",
    "    state, info = env.reset()\n",
    "    taxi_memory.reset(state)\n",
    "    if use_partial:\n",
    "        twist_level = np.random.randint(0, 4)\n",
    "        state = env_jump_before_pick(env, taxi_memory, twist_level) # spectial setting\n",
    "    else:\n",
    "        twist_level = np.random.randint(1, 4)\n",
    "        state = env_jump_after_pick(env, taxi_memory, twist_level) # spectial setting\n",
    "    state = get_state_tensor(state, taxi_memory, device)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(policy_net, state, device, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        old_visited = taxi_memory.visit_mask\n",
    "        if not done or done and (t == MAX_FUEL - 1):  # trucated\n",
    "            next_state = get_state_tensor(next_state, taxi_memory, device, action.item())\n",
    "        else:  # terminated\n",
    "            next_state = None\n",
    "        new_visited = taxi_memory.visit_mask\n",
    "        \n",
    "        augmented_reward = sum(old_visited) - sum(new_visited)\n",
    "        reward += augmented_reward * 2\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[\n",
    "                key\n",
    "            ] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            episodic_rewards.append(total_reward)\n",
    "            # plot_durations_and_rewards()\n",
    "            if (i_episode + 1) % log_steps == 0:\n",
    "                print(f\"[{i_episode}] Avg reward={np.average(episodic_rewards[-100:])}, Avg duration={np.average(episode_durations[-100:])}\")\n",
    "            break\n",
    "\n",
    "print(\"Complete\")\n",
    "plot_durations_and_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"DQN.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
